[%auto-animate.is-full]
== Gestion applicative

[.notes]
****
Maintenant que notre cluster est résilient sur 2 régions, il est temps de s'intéresser aux applications qui tournent dessus.
****

=== Par défaut

[.column]
--
plantuml::diagrams/040-deploy-without-anti-affinity.puml[format=svg,id=040-without-anti-affinity]
--

[.notes]
****
TODO manifeste Kubernetes basique sans affinité / anti-affinité

TODO Schéma déploiement sans affinité / anti-affinité sur 2 régions
****

=== Gestion des régions

[.column]
--
plantuml::diagrams/040-deploy-with-anti-affinity.puml[format=svg,id=040-with-anti-affinity]
--

[.notes]
****
TODO docs affinité / anti-affinité + exemples manifeste

TODO Schéma déploiement avec affinité / anti-affinité sur 2 régions
****

=== Et pour des besoins plus complexes ?

[.notes]
****
\= Stocker des données.

Parfois, nous dépendons d'opérateurs k8s, qui ont parfois leurs particularités.

Pourrait être simple, si Stockage répliqué, mais ce n'est pas le cas du client = 2 zones stockage par région, mais pas de réplication entre les régions = il faut gérer la réplication au niveau applicatif.
****

=== PostgreSQL

[.notes]
****
L'opérateur fait le travail, pour peu que l'on configure l'anti-affinité entre les régions

Anecdote : si API K8S KO, l'opérateur ne peut plus mettre à jour le service pour élire le nouveau principal
****

=== Elasticsearch

[.notes]
****
Cas particulier : n'utilise pas raft pour l'election de reader

Si on perd une région, on perd les données de cette région.
****

=== Kafka

[.notes]
****
Même problématique que API K8S = 2/1 ou 3/0 ? Pas possible de faire du 3/0, si on perd la mauvaise région, on perd tout.

TODO Schéma déploiement avec affinité / anti-affinité sur 2 régions

Procédure manuelle en cas de perte de région, en ajoutant un nouveau node Kafka, on provisionne un nouveau membre sur la région restante et le quorum peut de nouveau se faire si N > N/2

https://github.com/orgs/strimzi/discussions/11012
****
