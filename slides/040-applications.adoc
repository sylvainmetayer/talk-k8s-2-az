[%auto-animate.is-full]
== Gestion applicative

Un control plane haute disponibilité, mais un seul pod pour son application, ça n'a pas grand intérêt...

[.notes]
****
Maintenant que notre cluster est résilient sur deux régions, il est temps de s'intéresser aux applications qui tournent dessus. impact sur applications. partir de solution naive vers une proposition de solution
****

=== Par défaut

plantuml::diagrams/040-deploy-without-anti-affinity.puml[format=svg,id=040-without-anti-affinity]

[%notitle]
=== Gestion des régions

plantuml::diagrams/040-deploy-with-anti-affinity.puml[format=svg,id=040-with-anti-affinity,height=650px]


=== Affinité

[.qrcode]
qrcode::https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity[format="png", xdim=4]

[source%linenums,yaml,highlight=6-8|10-17]
----
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  affinity:
    podAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app.kubernetes.io/name
              operator: In
              values:
              - my-app
          topologyKey: topology.kubernetes.io/region
  containers: # [...]
----

[.notes]
****
Cas d'usage : latence faible, pour regrouper par exemple bdd et cache

Montrer que scheduler peut ignorer ici, car c'est une préférence
****

=== Anti-affinité

[.qrcode]
qrcode::https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity[format="png", xdim=4,class=qrcode]

[source%linenums,yaml,highlight=6-8|10-15]
----
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app.kubernetes.io/name
            operator: In
            values:
            - my-app
        topologyKey: topology.kubernetes.io/region
  containers: # [...]
----

[.notes]
****
Cas d'usage : pour répartir les pods sur les deux régions

Insister sur le fait que ici, c'est obligatoire et que le pod ne démarrera pas si scheduler non valide
****

=== Et pour du stateful ?

plantuml::diagrams/040-storage-presentation.puml[format=svg,id=040-storage-presentation,height=500]

[.notes]
****
= Stocker des données, bdd, faire des statefulset ne suffit pas, il faut gérer réplication données entre les régions.

Parfois, nous dépendons d'opérateurs k8s, qui ont parfois leurs particularités.

Pourrait être simple, si Stockage répliqué, mais ce n'est pas le cas du client = 2 zones stockage par région, mais pas de réplication entre les régions > il faut gérer la réplication des données au niveau applicatif. Du coup, nos PVC ne sont pas répliqués entre les régions et il faut gérer cela au niveau applicatif.
****

== PostgreSQL

https://cloudnative-pg.io/

[source%linenums,yaml,highlight=7-11]
----
---
apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: database
spec:
  affinity:
    enablePodAntiAffinity: true
    podAntiAffinityType: required # or preferred
    topologyKey: topology.kubernetes.io/region
  instances: 2
----

[.notes]
****
L'opérateur fait le travail, pour peu que l'on configure l'anti-affinité entre les régions

Configuration de la CRD pour indiquer que l'on répartit les pods sur les deux régions et on indique que l'anti-affinité est obligatoire selon le nombre d'instances demandées.

prefered, attention, nos instances pg peuvent se retrouver sur la même région
****

[%notitle]
=== ⚠️ Postgres

[.column]
--
[caption=,link=https://www.redhat.com/en/blog/build-your-kubernetes-operator-with-the-right-tool]
.⚠️
image::operator.webp[]
--

[.notes]
****
Anecdote : si API K8S KO, l'opérateur ne peut plus mettre à jour le service pour élire le nouveau principal. En effet, l'opérateur cherche à mettre à jour un service pour indiquer que le pod postgres restant doit devenir le pod principal et tente de mettre à jour le service Kubernetes correspondant. cependant, si pas d'API k8s... Pas de mise à jour du service !
****

// == Elasticsearch
//
// [.notes]
// ****
// Cas particulier : n'utilise pas raft pour l'election de reader
//
// Si on perd une région, on perd les données de cette région.
// ****
//
// === Ça pourrait être simple
//
// [.notes]
// ****
// Voir opérateur Elastic doc
// ****
//
// === Et si je ne peux pas utiliser l'opérateur ?
//
// [.notes]
// ****
// Helm chart bitnami
//
// Dans ce cas, on ne peut pas déployer 1 instance de chaque côté (car pair), 3 pas possible, car si on perds une région, on aura plus de quorum. 4, pas possible, car 2/2. 5 pas possible, car on se retrouvera à 2/3 et si on perds le 3, c'est mort. Du coup, on part sur 6 instances d'leastic, qui réduit le risque d'election de chacun et donc le cas nominal ne sera pas bloqué, et si on perds une région, on a toujours un quorum à 3 pour garantir une reprise.
// ****

[.columns]
== Kafka

[.column]
--
[link=https://fr.wikipedia.org/wiki/Fichier:Apache_Kafka_logo.svg]
image::kafka.svg[]
--

[.column]
--
.https://strimzi.io/
[link=https://strimzi.io/,caption=]
image::strimzi.png[]
--

[.notes]
****
Kafka est un système de file de messages distribué

https://kafka.apache.org/documentation/#gettingStarted

On déploie à l'aide d'un opérateur Strimzi, qui va nous permettre de gérer la réplication entre les régions.

Cependant, il y a 2 cas de figures
****

[.small-title]
=== Si vous êtes à la bourre... ⌚

plantuml::diagrams/040-kafka-zookeeper.puml[format=svg,id=040-kafka-zookeeper,height=650px]

[.notes]
****
Si vous êtes encore sous zookeeper, et bien déjà, sachez que vous ne pouvez plus mettre à jour vers de nouvelles versions, car Kraft est le mode par défaut pour les éléctions de leader.

**⚠️ Depuis Kafka 4.0, le mode de fonctionnement par défaut est Kraft (Kafka Raft). Il n'y a plus de fonctionnement possible avec Zookeeper ⚠️**

Pourquoi on a besoin de Zookeeper ? Parce que c'est lui qui gère les élections de leader entre les brokers. En effet, si on perd une région, il faut élire un nouveau leader, et pour cela, il faut que Zookeeper soit accessible. Ainsi, la consommation/production dans les topics n'est possible que si Zookeeper est disponible, car c'est lui qui indiquera au broker quelle est la partition leader.

https://kafka.apache.org/documentation/
****

[.small-title]
=== Et si je suis à jour ?

plantuml::diagrams/040-kafka.puml[format=svg,id=040-kafka,height=650px]

[.notes]
****
Si vous avez déjà migré sur Kraft (ça vous rappelle quelques choses ?), vous ne devriez rien avoir à faire, si ce n'est faire attention à la configuration de vos topics, afin de s'assurer de la bonne réplications de vos données. En effet, l'election est géré par les brokers eux-mêmes et non plus par Zookeeper. Il n'y a donc plus de dépendance à Zookeeper pour l'élection des leaders, mais il faut tout de même s'assurer que les brokers sont bien répartis sur les deux régions.

TODO POC test Kafka (cf Jérôme)

Cependant, attention à la configuration applicative de vos topics, sinon, vous pourriez avoir un kafka certes disponible, mais les données de vos topics ne seront pas répliquées entre les deux régions.
****


=== ⚠️ Pensez à vos topics !

[.qrcode]
qrcode::https://strimzi.io/docs/operators/latest/configuring.html[format="png", xdim=4]

[source,yaml%linenums,highlight=7-11]
----
---
kind: Kafka
metadata:
  name: kafka
spec:
  kafka:
    rack:
      topologyKey: topology.kubernetes.io/region
    config:
      replica.selector.class: org.apache.kafka.common.replica.RackAwareReplicaSelector
      min.insync.replicas: 2
----

[.notes]
****
Activer la sélection basée sur les racks : RackAwareReplicaSelector dans replica.selector.class. Cela garantit que Kafka essaiera de placer les réplicas sur des racks (ou régions) différents.

Même problématique que API K8S = 2/1 ou 3/0 ? Pas possible de faire du 3/0, si on perd la mauvaise région, on perd tout. On va donc préférer avoir 2 kafka sur chaque région, et assurer une réplication minimum de 2 pour chaque message dans les topics, afin de s'assurer d'avoir une copie du message dans chaque région.


https://github.com/orgs/strimzi/discussions/11012
****
